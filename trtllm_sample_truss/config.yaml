build:
  arguments:
    engine_repository: "baseten/llama-7b-no-quant-tp1-trtllm-engine"
    tokenizer_dir: "NousResearch/Llama-2-7b-hf"
  model_server: TRTLLM
environment_variables: {}
external_package_dirs: []
model_metadata: {}
model_name: modelx
python_version: py39
requirements: []
resources:
  accelerator: null
  cpu: '1'
  memory: 2Gi
  use_gpu: false
secrets: {}
system_packages: []
